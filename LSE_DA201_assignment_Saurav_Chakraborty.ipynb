{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97fe015",
   "metadata": {},
   "source": [
    "# DA201: Data Analytics using Python\n",
    "# Main Assignment\n",
    "\n",
    "## Introduction\n",
    "\n",
    "I will be using this Jupyter Notebook to produce the data analysis for the assignment. Insights from the analysis will be recorded on this notebook too alongside the analytical code snippet. A version of this notebook along with all associated datafiles have been uploaded into a special repo within my GitHub Page. It is linked further down below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b481a53",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b971f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries required.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from datetime import datetime\n",
    "from matplotlib import rcParams\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "# Set plotting options.\n",
    "sns.set_palette(\"pastel\")\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "# Setting up date-parser using a lambda.\n",
    "d_parser = lambda x: pd.datetime.strptime(x, '%d/%m/%Y')\n",
    "\n",
    "# Import the CSV files containing the data required for the Assignment.\n",
    "# Running the date-parser\n",
    "tweets = pd.read_csv('tweets.csv')\n",
    "cov = pd.read_csv('covid_19_uk_cases.csv', parse_dates=['Date'], date_parser=d_parser)\n",
    "vac = pd.read_csv('covid_19_uk_vaccinated.csv', parse_dates=['Date'], date_parser=d_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6258019",
   "metadata": {},
   "source": [
    "## Hyperlinking URL to My GitHub Repo\n",
    "\n",
    "- All work relating to this Assignment can be found on my [GitHub Pages](https://github.com/SauravChakers?tab=repositories).\n",
    "\n",
    "- Screenshot demo. \n",
    "\n",
    "!['My Github screenshot](https://github.com/SauravChakers/Data_Analytics_using_Python/blob/main/MyGitHub%20Screenshot.png?raw=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ebf28f",
   "metadata": {},
   "source": [
    "## What is GitHub?\n",
    "- A free online resource for organising, storing and sharing code and related work with others.\n",
    "- Cloud storage for any file type but is most often used for code files and related supporting documents.\n",
    "- A platform to collaborate, network and pitch work amongst developers in real time.\n",
    "- The most widely used version control system amongst developers and coders.\n",
    "\n",
    "## How does GitHub add value?\n",
    "- It has a built-in version control feature which can then be used to:\n",
    "    - Track changes and those responsible for the changes. \n",
    "    - Create localised copy away from production version where upgrades or fixes can first be tested ahead of implementation.\n",
    "    - Roll back to previous versions. \n",
    "- Version control is useful on collaborative projects where simultaneous work may be required.\n",
    "- It can be used to track progressive performance of coders.\n",
    "- As it is open source, projects can potentially draw upon resources of the entire community of coders.\n",
    "- Provide a simple back up of work in the cloud asides from any local networks and hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bd781",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c081f5",
   "metadata": {},
   "source": [
    "### Tweets DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42849a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows and columns.\n",
    "print(tweets.shape)\n",
    "\n",
    "# Can see 3960 rows and 21 columns.\n",
    "# Implies we have 3960 tweets in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1266928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data types in the DataFrames.\n",
    "print(tweets.dtypes)\n",
    "\n",
    "# Can see this is NOT all text data.\n",
    "# Should be able to rank the most favourite or retweeted content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0039437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information on the contents of the columns.\n",
    "tweets.info()\n",
    "\n",
    "# Looks like the main content of each tweet is contained in the column 'text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a54dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 rows to see whats in each column.\n",
    "tweets.head()\n",
    "\n",
    "# Confirmed that 'text' column contains the main content of the tweets.\n",
    "# Contains # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 5 rows.\n",
    "print(tweets.tail())\n",
    "\n",
    "# Can see from heads and tails that there are 21 columns and no footer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c83a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine null values.\n",
    "tweets.isnull().sum()\n",
    "\n",
    "# Can see 1 row may have all columns blanks.\n",
    "# Can see at 830 tweets were considered as possibly sensitive by Twitter algos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a3b50",
   "metadata": {},
   "source": [
    "### Cases DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine key characteristics of each DataFrame.\n",
    "# View the first 5 rows.\n",
    "print(cov.head())\n",
    "\n",
    "# Can see index value 0 is header.\n",
    "# Date Values are in rows i/o columns.\n",
    "# Time-series often easier if time values are on columns.\n",
    "# Lots of geographical data; need to consider usefulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 5 rows.\n",
    "print(cov.tail())\n",
    "\n",
    "# All rows until the end have changes in row values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3907cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows and columns.\n",
    "print(cov.shape)\n",
    "\n",
    "# Can see 7584 rows and 21 columns.\n",
    "# Date format is dd/mm/yyyy\n",
    "# Changing to date parser at load -in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data types in the DataFrames.\n",
    "print(cov.dtypes)\n",
    "# Can see date parser at load-in worked.\n",
    "# Date in correct pandas format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine null values.\n",
    "cov.isnull().sum()\n",
    "\n",
    "# Can see very few missing values.\n",
    "# Will determine what to do with them later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750695b",
   "metadata": {},
   "source": [
    "### Vaccinated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine key characteristics of each DataFrame.\n",
    "# View the first 5 rows.\n",
    "print(vac.head())\n",
    "\n",
    "# Can see index value 0 is header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 5 rows.\n",
    "print(vac.tail())\n",
    "\n",
    "# Perhaps the last rows dont have any data; need to check.\n",
    "# Will change dates to columns i/o rows here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows and columns.\n",
    "print(vac.shape)\n",
    "\n",
    "# Can see 7584 rows and 11 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3481c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the data types in the DataFrames.\n",
    "print(vac.dtypes)\n",
    "\n",
    "# Can see date parsing successful here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8133d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces in columns name\n",
    "vac.columns = vac.columns.str.replace(' ','_')\n",
    "\n",
    "# Makes running formulas easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786f169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine null values.\n",
    "vac.isnull().sum()\n",
    "\n",
    "# Can see no missing values.\n",
    "# Given all column names are shared with Cases it may be a good idea to merge the two DataFrames.\n",
    "# Do we need all this geographical data? Removing it might make the DataFrame more resource efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856789e4",
   "metadata": {},
   "source": [
    "## Filtering the data for region Gibraltar\n",
    "\n",
    "I will filter the data for a small subset.\n",
    "This should give me an idea of how to approach the wider data set once it has been wrangled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a10b3",
   "metadata": {},
   "source": [
    "### Cases Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c26aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Are there default column names?\n",
    "list(cov.columns)\n",
    "\n",
    "# Yes, headers are defined as default column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792e4bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter the data by Province/State = Gibraltar\n",
    "# Gib is a boolean variable with True or False in it.\n",
    "Gib = cov[cov['Province/State'] == 'Gibraltar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating daily changes to Deaths, Cases, Recovered.\n",
    "# Create new Column for each variable with 'Day' prefix.\n",
    "# Calculate the daily change using shift(1) and subtract functions.\n",
    "Gib['Day_Deaths']=Gib['Deaths'] -Gib['Deaths'].shift(1)\n",
    "Gib['Day_Cases']=Gib['Cases'] -Gib['Cases'].shift(1)\n",
    "Gib['Day_Recovered']=Gib['Recovered'] -Gib['Recovered'].shift(1)\n",
    "\n",
    "# There may be a neater way to do this calculation using loc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking when Gibraltar first reported a case.\n",
    "# This should should 1 in Daily_Cases column.\n",
    "# Need to filter in column Cases > 0\n",
    "Filt_Gib = Gib[Gib['Cases'] > 0] \n",
    "Filt_Gib\n",
    "\n",
    "\n",
    "\n",
    "# Proof that the daily changes calculations above work!\n",
    "# First case reported on 4 Mar 2020.\n",
    "# Total 97 Deaths in Gibraltar for the period of the data.\n",
    "# Total 5727 Cases in Gibraltar for the period of the data.\n",
    "# See that Daily Cases at start of period of data was very low.\n",
    "# See that Daily Cases at end of period of data still comparatively high.\n",
    "# Note multiple peaks and troughs in daily data through the period of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167685c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When was first death recorded in Gibraltar?\n",
    "# Need to filter in column Deaths > 0\n",
    "Filt_Gib = Gib[Gib['Deaths'] > 0] \n",
    "Filt_Gib\n",
    "\n",
    "# See that first Death reported in Gibraltar on 11 Nov 2020\n",
    "# Daily cases at this time of c. mid 20s not much different to daily cases at end of period of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afdd1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aggregate Data by month.\n",
    "Gib_monthly = Gib.set_index('Date', inplace=True)\n",
    "Gib.index = pd.to_datetime(Gib.index)\n",
    "Gib.resample('1M').sum()\n",
    "\n",
    "# This is much easier to extract insights out of.\n",
    "# Deaths in winter 2020/21 starting from Oct 2020 & peaking in Jan 2021 with 70 deaths in that month.\n",
    "# Dramartic decline in Deaths after Feb 2021. Be interesting to investigate link with vaccine dataframe.\n",
    "# Sharp rise in cases from Sept 2020 after which it becomes even more dramatic in the winter of 2020/21.\n",
    "# Cases remain very high until Sept 2021 and but declines significantly in Oct 2021.\n",
    "# Sharp rise in daily cases during Dec 2020 & Jan 2021. \n",
    "# Stabilises thereafter barring a spike in July & Aug 2021.\n",
    "# Looks like strong correlation between peaks of cases, deaths and hospitalisation numbers albeit with some lags.\n",
    "# Note that aggregating by month changes the deaths, cases, recovered and hospitalised.\n",
    "# Will need to account for these changed ahead of analysis in the wider data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8cd7a",
   "metadata": {},
   "source": [
    "### Vaccinated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47053d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data by Province/State = Gibraltar\n",
    "# Gib is a boolean variable with True or False in it.\n",
    "Gib = vac[vac['Province/State'] == 'Gibraltar']\n",
    "Gib\n",
    "\n",
    "# First_Dose, Second_Dose=Vaccinated are daily numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating difference between 'First Dose' and 'Second Dose' values.\n",
    "# This will represent the individuals who have taken First Dose but not Second Dose.\n",
    "# Call this 'Part_Vac'.\n",
    "Gib['Part_Vac']=Gib['First_Dose'] - Gib['Second_Dose']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking when Gibraltar first administered the first dose.\n",
    "# This should should at least 1 in First_Dose column.\n",
    "# Need to filter in column First_Dose > 0\n",
    "Filt_Gib = Gib[Gib['First_Dose'] > 0] \n",
    "Filt_Gib\n",
    "\n",
    "# Can see first day of vaccine being rolled out was 11 Jan 2021.\n",
    "# Strange that there were 2596 second doses also administered on that day. \n",
    "# So where are the first doses for these?\n",
    "# Too many data points to visually discern any meaningful insights.\n",
    "# Need to aggregate data in monthly intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543240dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Data by month.\n",
    "Gib_monthly = Gib.set_index('Date', inplace=True)\n",
    "Gib.index = pd.to_datetime(Gib.index)\n",
    "Gib.resample('1M').sum()\n",
    "\n",
    "# Large increase in the number of first doses being adminstered in the first few months.\n",
    "# Stabilises from Mar to Jun 2021 and then declines sharply.\n",
    "# One month lag between peaks of first & second dose.\n",
    "# Part vaccinated declines sharply as second dose adminstrations start to outstrip first dose.\n",
    "# No meaningful insight aside from above on part vaccinated.\n",
    "# Time Series & correlations might make it easier to understand.\n",
    "# Need to check aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying pairplot to visualise relationships between the variables.\n",
    "sns.pairplot(Gib)\n",
    "\n",
    "# Too many columns.\n",
    "# Very noisy visualisation of Dataframe.\n",
    "# Will return on the wider data set once it has been wrangled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa32b9",
   "metadata": {},
   "source": [
    "## Expectations on the wider data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b8b54",
   "metadata": {},
   "source": [
    "Filtering the Data so far for just Gibraltar was an useful exercise. For the fuller data, this exercise has shown:\n",
    "- 1. Where calculated columns need to be added & manipulated to incorporate changes made during aggregation.\n",
    "- 2. Some columns which can be deleted as they contain no meaningful data for our analysis.\n",
    "- 3. Daily data is too noisy to gather any insights.\n",
    "- 4. Data will need to be resampled into monthly output.\n",
    "- 5. The two datasets Cov & Vac should be merged.\n",
    "- 6. Total numbers are useful to highlight local peaks and troughs as well factors such as seasonality.\n",
    "- 7. Trends over time offer broader insights into interdependence between variable.\n",
    "- 8. Full analysis will need to consider both totals as well as timeseries.\n",
    "- 9. Visualisation not useful at this stage as there is too much noise in the data: too many unncesesary columns, very large variation in data scales and very likely the presence of outliers.\n",
    "- 10. We now know how to wrangle the data best such that we may anlyse it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8027bff",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea345b8e",
   "metadata": {},
   "source": [
    "### Merging Cases & Vaccinated DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7bf3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'covid' DataFrame will merge 'cov' and 'vac' DataFrames.\n",
    "covid = pd.merge(cov, vac, left_index=True, right_index=True)\n",
    "\n",
    "print(covid.shape)\n",
    "print(cov.shape)\n",
    "print(vac.shape)\n",
    "\n",
    "# Can see that there are the same number of rows in covid as there were in cov and vac.\n",
    "# Can see that the number of columns in covid add up to the total columns in cov and vac."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27de44",
   "metadata": {},
   "source": [
    "### Cleaning up the Merged DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9035c",
   "metadata": {},
   "source": [
    "#### Shape of the Merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first pull up the column names in 'covid' to identify duplications & redundancies.\n",
    "# Will also determine data types at the same time.\n",
    "print(covid.dtypes)\n",
    "\n",
    "# Delete all duplicate and redundant columns from merged DataFrame 'covid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any duplicated columns & dropping them.\n",
    "covid = covid.loc[:,~covid.apply(lambda x: x.duplicated(),axis=1).all()].copy()\n",
    "print(covid.dtypes)\n",
    "\n",
    "# All columns which are duplicated are deleted/dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6044919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete redundant columns from the dataframe 'covid'.\n",
    "# These columns contain no useful information for our analysis.\n",
    "covid_cleaned = covid.drop(covid.columns[[1, 2, 3, 4, 5, 6]], axis=1) \n",
    "# Realigning names of some columns in covid_cleaned to original dataset.\n",
    "covid_cleaned.rename(columns={\"Province/State_x\": \"Province/State\", \n",
    "                              \"Date_x\": \"Date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types & shape of covid\n",
    "print(covid_cleaned.dtypes)\n",
    "print(covid_cleaned.shape)\n",
    "\n",
    "# Output as designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552d6aa",
   "metadata": {},
   "source": [
    "#### Dealing with Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing/null values.\n",
    "covid_cleaned.isnull().sum()\n",
    "\n",
    "# Can see 2 rows have nulls values\n",
    "# Ties up with earlier wrangling done on Cases DataFrame.\n",
    "# Selective 0 values should be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22526dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows which does not contain any values in the columns of interest.\n",
    "# This reduces demand for computational power.\n",
    "covid_clean = covid_cleaned.drop(covid_cleaned[(covid_cleaned['Deaths'] == 0) & \n",
    "                            (covid_cleaned['Cases'] == 0) & \n",
    "                            (covid_cleaned['Recovered'] == 0) & \n",
    "                            (covid_cleaned['Hospitalised'] == 0) &\n",
    "                            (covid_cleaned['Vaccinated'] == 0) & \n",
    "                            (covid_cleaned['First_Dose'] == 0)].index)\n",
    "\n",
    "# covid_clean is the new baseline DataFrame to use.\n",
    "# Will use this for all analysis going forward.\n",
    "print(covid_clean.shape)\n",
    "# Reduces rows from 7584 to 6942.\n",
    "# Will enhance efficiency of DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the 2 rows with the null values.\n",
    "covid_clean[covid_clean.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets examine the DateFrame around these rows.\n",
    "# Will help determine how these null values should be replaced.\n",
    "covid_clean.iloc[750:758]\n",
    "\n",
    "# No changes to any column except Hospitalised on these two days.\n",
    "# We can backfill Hospitalised for the missing values given values are rising.\n",
    "# We can replace other NaN values either with backfill or forward fill.\n",
    "# As there is no change in the values either direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe543bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with backward fill in Hospitalised. \n",
    "covid_clean['Hospitalised'] = covid_clean['Hospitalised'].fillna(method='bfill')\n",
    "# Replace other Null values\n",
    "covid_clean = covid_clean.fillna(method='ffill')\n",
    "#Check for any other null values\n",
    "covid_clean[covid_clean.isna().any(axis=1)]\n",
    "\n",
    "# No more null values left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the same rows to verify what values have been inserted.\n",
    "covid_clean.iloc[750:758]\n",
    "\n",
    "# Replacements successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab61e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the DataFrame.\n",
    "duplicateRows = covid_clean[covid_clean.duplicated()]\n",
    "\n",
    "#view duplicate rows\n",
    "duplicateRows\n",
    "\n",
    "# No Duplicates found. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4b6ed",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffe46a",
   "metadata": {},
   "source": [
    "#### Adding Calculated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc14bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need to identify the difference between application of First & Dose.\n",
    "# This represents the individuals who have taken the First Dose of the vaccine but not Second Dose.\n",
    "# This is the target group for the marketing campaign.\n",
    "# Call this 'Part_Vac'.\n",
    "covid_clean['Part_Vac'] = covid_cleaned['First_Dose'] - covid_cleaned['Vaccinated']\n",
    "\n",
    "# Top row in all calculated columns will return Null value as there is no preceeding value.\n",
    "# This is expected when using shift function for calculations which requires a preceedig value to perform the calculation.\n",
    "# Replace this row NaN values with 0.\n",
    "covid_clean.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample datetime objects into month and year.\n",
    "# Will be needed for aggregate functions.\n",
    "# Will reduce the frequency of the data.\n",
    "# Create new columns Month & Year.\n",
    "\n",
    "covid_clean['Month'] = covid_clean['Date'].dt.month\n",
    "covid_clean['Year'] = covid_clean['Date'].dt.year\n",
    "covid_clean['Week'] = covid_clean['Date'].dt.week\n",
    "covid_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf6d09",
   "metadata": {},
   "source": [
    "#### Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79393282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the index to Date.\n",
    "# This converts the DataFrame into a timeseries data.\n",
    "covid_clean.set_index('Date', inplace=True)\n",
    "covid_clean.info()\n",
    "\n",
    "# With Date as Index, we can now filter DataFrame with Dates.\n",
    "# Much easier to work with for time series data in this format.\n",
    "# No need to transpose dates into column values.\n",
    "# Avoids creating a very wide DataFrame which is potentially more unwieldy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5987137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic visualisation to check if time series created.\n",
    "covid_clean.Hospitalised.plot(title=\"Hospitalisations over time\")\n",
    "plt.tight_layout(); plt.show\n",
    "\n",
    "# Shows that creation of time series is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the highest number of individuals who have recieved first dose but not second dose.\n",
    "# This is a point in time static identification of this value.\n",
    "# This is available in Part_Vac Column\n",
    "# Identifying max value in Part Vac_Column & returning all corresponding row values to identify Province/State\n",
    "\n",
    "covid_clean[covid_clean['Part_Vac']==covid_clean['Part_Vac'].max()]\n",
    "\n",
    "# So Gibraltar had the highest number of individuals who had received the first dose but not second dose\n",
    "# This was on 20 Mar 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for peak roll out of first dose\n",
    "# This period represents the peak month for the administration of the first dose.\n",
    "covid_clean.groupby('Month')['Part_Vac'].sum()\n",
    "\n",
    "# We can see that this was Month 2 = Feb 2021\n",
    "# On a monthly aggregate, the highest number of individuals who received first dose but not second dose was in 02/21.\n",
    "# The peak value for such people = Part_Vac was 10,657,478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how Part_Vac evolved over time in each 'Province/State'.\n",
    "# Group the DataFrame by multiple Groups as columns.\n",
    "\n",
    "covid_clean.groupby(['Month','Province/State'], as_index=False)['Part_Vac'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame for multiple functions.\n",
    "# Region wide month on month break down of total and average first & second dose administered.\n",
    "# Recall that Second dose = Vaccinated.\n",
    "covid_clean.groupby(['Month','Year','Province/State'], as_index=False)['First_Dose', 'Vaccinated'].agg(['sum', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying highest % of individuals who have received first dose but not second dose.\n",
    "# Will need to add calculated column.\n",
    "# Not clear what the denominator in this function should be\n",
    "# Individuals who received first dose but not second dose = Part_Vac.\n",
    "# The denominator is individuals who have recieved just First Dose.\n",
    "# First Dose is a proxy for eligibility for  receiving Second Dose too. \n",
    "# This is a good indicator of \"riskiness\" of any row of data.\n",
    "# In theory this is where vaccination campaign needs to target as least people are fully vaccinated vs eligibility\n",
    "\n",
    "# Inserting Calculated Column\n",
    "covid_clean['Ratio_of_Int'] = ((covid_clean['Part_Vac'])*100 / covid_clean['First_Dose'])\n",
    "\n",
    "# The lower the value in this column, the worse it is!\n",
    "\n",
    "# Identifying max value in % at Risk column & returning all corresponding row values to identify Province/State\n",
    "covid_clean[covid_clean['Ratio_of_Int']==covid_clean['Ratio_of_Int'].min()]\n",
    "\n",
    "# This is in the Province/State = '\tSaint Helena, Ascension and Tristan da Cunha'\n",
    "# This is a point in time observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show monthly aggregated for Ratio_of_Int.\n",
    "covid_clean.groupby('Month')['Ratio_of_Int'].sum()\n",
    "\n",
    "# Confirms again that Month 2 = Feb 2021 was peak month when most people had received just the first dose.\n",
    "# In this example, this feature is called Peak Ratio of Interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Ratio of people who have received seond dose over the first dose.\n",
    "# Calling this 'First Percentage'\n",
    "# Inserting Calculated Column\n",
    "covid_clean['First_Percentage'] = (covid_clean['First_Dose']) / (covid_clean['Vaccinated'])\n",
    "\n",
    "# See what the DataFrame now looks like after addition of all calculated columns.\n",
    "covid_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea2998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns so that they have a more logical order in the DataFrame.\n",
    "# Makes the DataFrame look neater too.\n",
    "\n",
    "covid_clean = covid_clean[['Week', 'Month', 'Year', 'Province/State', 'Deaths', 'Cases', 'Recovered', \n",
    "                           'Hospitalised', 'First_Dose', 'Part_Vac', 'Vaccinated', \n",
    "                           'First_Percentage', 'Ratio_of_Int']]\n",
    "\n",
    "# Check how the rearranged DataFrame looks.\n",
    "covid_clean\n",
    "\n",
    "# Confirmed that the columns now flow more logically and looks neater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see if there is are differences over time in application of First and Second Dose.\n",
    "# The difference between application of First & Second Dose is available in Part_Vac Column\n",
    "# Lets pull up how the difference between application of First and Second Dose has evolved over monthly intervals.\n",
    "# Resample the data for Monthly aggregate of this column.\n",
    "t_series_Part_Vac=covid_clean['Part_Vac'].resample('M').sum()\n",
    "t_series_Part_Vac\n",
    "\n",
    "# We can see that Vaccine Roll out was in  Jan 2021\n",
    "# We can see that until Mar 2021, there were many more First Doses administered vs Second Dose.\n",
    "# From April 2021, we can see that the trend reverses.\n",
    "# More Second Doses being adminstered than First Dose.\n",
    "# This is the period when the vaccinated numbers will rise.\n",
    "# Quickly resample the dataframe to confirm below.\n",
    "# Interesting to note that again in Oct 2021, there is a period when first doses > second dose.\n",
    "# Likely that those who didnt take the vaccine in the first wave are now complying with vaccine mandate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5523e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a basic visualisation\n",
    "%matplotlib inline\n",
    "t_series_Part_Vac.plot()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Visualisation is basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06bf11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Resampling the entire dataframe into Monthly aggregates.\n",
    "# This is for across all the regions in the UK.\n",
    "cov_agg = covid_clean.resample('M').agg({'Cases': 'sum', 'Deaths': 'sum', 'First_Dose': 'sum', \n",
    "                                         'Vaccinated': 'sum', 'Part_Vac': 'sum', 'Ratio_of_Int': 'sum',\n",
    "                                        'First_Percentage': 'sum'})\n",
    "# Lets see the DataFrame\n",
    "cov_agg\n",
    "\n",
    "# We are missing regional variations but good overview of how each variable evoloved on a monthly basis.\n",
    "# So we can see that end of 2020, there were 2,488,780 cases of Covid-19 reported across the UK.\n",
    "# By the end of 2020, over 73,512 people had died from Covid-19 infection across the UK.\n",
    "# Death rates continue to rise sharply until Mar 2021.\n",
    "# This coincides with above where we saw that the bulk of the population was partly vaccinated by then.\n",
    "# Suggests that even just the first dose of the vaccines may have helped to reduce deaths.\n",
    "# At the very least, they helped reduce the rate of increase of deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a6a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A basic visualisation of the key columns will help really emphasise findings above. \n",
    "# Columns to be visualised are \"Deaths\", \"Cases\", \"First_Dose\"\n",
    "# Big differences in scale of range of data so will plot on log\n",
    "# Set out the lineplot variables.\n",
    "g=sns.lineplot(x=\"Date\", y=\"Deaths\", data=cov_agg)\n",
    "g=sns.lineplot(x=\"Date\", y =\"Cases\", data=cov_agg)\n",
    "g=sns.lineplot(x=\"Date\", y =\"First_Dose\", data=cov_agg)\n",
    "\n",
    "# Set y axis scale.\n",
    "g.set_yscale('log')\n",
    "\n",
    "# Set out chart title.\n",
    "g.set_title('TimeSeries on Key Aggregate Variables')\n",
    "\n",
    "# Set out legend.\n",
    "g.legend(['Deaths', 'Cases', 'First_Dose'])\n",
    "\n",
    "# Confirms that Deaths flatline once the vaccine is introduced.\n",
    "# Need to do better on scales etc on these visualisations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280d207",
   "metadata": {},
   "source": [
    "### Finding and cleaning for Outliers using z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f1f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before handling outliers, I will identify them for each column.\n",
    "# Boxplot is best for visualising outliers.\n",
    "rcParams['figure.figsize'] = 40,20\n",
    "sns.boxplot(data=covid_clean).set(title='Chart1')\n",
    "sns.despine(offset=10, trim=True);\n",
    "\n",
    "# Column Values are on too many orders of magnitude to visualise in one chart.\n",
    "# For visualisation, Use log scales to negate the differences in scales of the column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c332f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using log scale to replot.\n",
    "g=sns.boxplot(data=covid_clean)\n",
    "g.set_yscale('log')\n",
    "g.set_title('Boxplot of Key Variables on log scale')\n",
    "plt.show()\n",
    "\n",
    "# Visualisation much clearer.\n",
    "# Lots of values outside upper fence.\n",
    "# Values outside upper fence = 'outliers'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04b115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for distribution tails to measure \"quality of outliers\".\n",
    "# Violinplot is best for visualising distribution tails. \n",
    "# Multi-modal distribution tails are not outliers.\n",
    "# Remove unnecessary and dependant variables to improve the visualisation's focus.\n",
    "covid_clean2 = covid_clean.drop(['Week', 'Month', 'Year', 'Province/State', \n",
    "                                 'Part_Vac', 'First_Percentage', 'Ratio_of_Int'], axis=1)\n",
    "\n",
    "# Normalise the modified dataframe to narrow down the range of the magnitude of the variables\n",
    "# Doesnt alter statistical properties of the variables.\n",
    "normalized_covid_clean2=(covid_clean2-covid_clean2.mean())/covid_clean2.std()\n",
    "\n",
    "# Set out the plot.\n",
    "g=sns.violinplot(data=normalized_covid_clean2, showmedians=True, showmeans=True)\n",
    "g.set_title('Chart2')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Can see that all columns are fat tailed although Hospitalised less so.\n",
    "# Some evidence of multi-modality in Deaths, Cases & Recovered.\n",
    "# A column by column differentiate approach to outlier cleaning warranted.\n",
    "# This will maintain distribution qualities such as multimodality.\n",
    "# Beyond this, outliers can be replaced with interpolated values based on values from neighbouring rows.\n",
    "# Outlier cleaning warranted across all columns albeit by different levels of aggresiveness.\n",
    "# Deaths and Cases need most aggressive treatment as they have the most data points outside the box & most fat tailed.\n",
    "# Hospitalised needs least agressive cleaning as it is comparitively thin tailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9986aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Z-Score Method to replace outliers.\n",
    "# Outliers replaced with neighbouring row interpolation.\n",
    "d = covid_clean.Deaths\n",
    "m = ((d - d.mean()) / d.std()).abs() > 1.5\n",
    "covid_clean['Deaths'] = covid_clean['Deaths'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d851a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the Cases column.\n",
    "c = covid_clean.Cases\n",
    "m = ((c - c.mean()) / c.std()).abs() > 1.5\n",
    "covid_clean['Cases'] = covid_clean['Cases'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78322d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the Recovered column.\n",
    "r = covid_clean.Recovered\n",
    "m = ((r - r.mean()) / r.std()).abs() > 1.5\n",
    "covid_clean['Recovered'] = covid_clean['Recovered'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the Hospitalised column.\n",
    "h = covid_clean.Hospitalised\n",
    "m = ((h - h.mean()) / h.std()).abs() > 1.5\n",
    "covid_clean['Hospitalised'] = covid_clean['Hospitalised'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the Vaccinated column.\n",
    "v = covid_clean.Vaccinated\n",
    "m = ((v - v.mean()) / v.std()).abs() > 1.5\n",
    "covid_clean['Vaccinated'] = covid_clean['Vaccinated'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd50a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the First_Dose column.\n",
    "fd = covid_clean.First_Dose\n",
    "m = ((fd - fd.mean()) / fd.std()).abs() > 1.5\n",
    "covid_clean['First_Dose'] = covid_clean['First_Dose'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db59ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the Part_Vac column.\n",
    "pv = covid_clean.Part_Vac\n",
    "m = ((pv - pv.mean()) / pv.std()).abs() > 1.5\n",
    "covid_clean['Part_Vac'] = covid_clean['Part_Vac'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the Ratio_of_Int column.\n",
    "roi = covid_clean.Ratio_of_Int\n",
    "m = ((roi - roi.mean()) / roi.std()).abs() > 1.5\n",
    "covid_clean['Ratio_of_Int'] = covid_clean['Ratio_of_Int'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the outlier replacement to the First_Percentage column.\n",
    "fp = covid_clean.First_Percentage\n",
    "m = ((fp - fp.mean()) / fp.std()).abs() > 1.5\n",
    "covid_clean['First_Percentage'] = covid_clean['First_Percentage'].mask(m).interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52720b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise after outliers have been replaced.\n",
    "# Using boxplot first.\n",
    "g=sns.boxplot(data=covid_clean)\n",
    "g.set_yscale('log')\n",
    "g.set_title('Boxplot of cleaned key variables on logscale ')\n",
    "plt.show()\n",
    "\n",
    "# As expected far fewer outliers across the board when compared to Chart1 above.\n",
    "# Can see that the variables/columns have data on a very wide magnitude of range from the log scale below.\n",
    "# Need to change the data such that all y axis values fall within a narrower range.\n",
    "# This is called re-scaling/normalisation.\n",
    "# For now will not normalise but instead use log scales for visualisation.\n",
    "# Will revisit normalisation later if warranted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise distributions after outliers have been replaced.\n",
    "# Using violinplot again.\n",
    "# Remove unnecessary date parsing and calculated columns to improve the visualisation's focus.\n",
    "covid_clean2 = covid_clean.drop(['Week', 'Month', 'Year', 'Province/State', \n",
    "                                 'Part_Vac', 'First_Percentage', 'Ratio_of_Int'], axis=1)\n",
    "\n",
    "# Normalise the modified dataframe to narrow down the range of the magnitude of the variables\n",
    "# Doesnt alter statistical properties of the variables.\n",
    "normalized_covid_clean2=(covid_clean2-covid_clean2.mean())/covid_clean2.std()\n",
    "\n",
    "# Setting up the plot.\n",
    "g=sns.violinplot(data=normalized_covid_clean2, showmedians=True, showmeans=True)\n",
    "g.set_title('Violinplot of cleaned key variables')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Unfortunately this vis isnt great to much sense of the transformation versus earlier in Chart 2.\n",
    "# This is because some time series related columns such as Week, Month etc are corrupting the visualisation.\n",
    "# Clean up some unnecessary coloumns and plot again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47b22a",
   "metadata": {},
   "source": [
    "### Visualise & Identify Initial Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting vertical Bar Graph\n",
    "# Plotting First Percentage as a time series.\n",
    "# Setting the style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "f, g = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Grouped by Province/State\n",
    "df=covid_clean.groupby(['Month','Province/State'], as_index=False)['First_Percentage'].sum()\n",
    "\n",
    "\n",
    "g=sns.barplot(x = covid_clean[\"Month\"], y = covid_clean[\"First_Percentage\"], data= df)\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "g.set(xlim=(0, 24), ylabel=\"\",\n",
    "       xlabel=\"Sum of First Percentage by Month in 2021\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "# Setting size of display\n",
    "rcParams['figure.figsize'] = 25,15\n",
    "\n",
    "# Display\n",
    "plt.show()\n",
    "\n",
    "# Saving the Seaborn Figure:\n",
    "plt.savefig('t-series_First_Percentage.png')\n",
    "\n",
    "# Shows that there is a slow rise in Number of individuals who are eligible for the second dose who havent taken it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by Province/State and Date, and aggregate the death count.\n",
    "# Creating the df first per above.\n",
    "\n",
    "df=covid_clean.groupby(['Month','Province/State'], as_index=False)['Deaths'].sum()\n",
    "\n",
    "# Setting the size\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "\n",
    "# Setting the style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Visualising trend of deaths across all regions\n",
    "g=sns.lineplot(x=\"Month\", y=\"Deaths\", data=df, hue='Province/State')\n",
    "g.set_title('Deaths across regions over time')\n",
    "\n",
    "# Very clear that death count from \"Others\" is skewing the data.\n",
    "# Plot again on log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c1653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting on log scale to remove the effect of vast difference in scale of numbers within the variable, Deaths. \n",
    "# Visualising trend of deaths across all regions\n",
    "\n",
    "g=sns.lineplot(x=\"Month\", y=\"Deaths\", data=df, hue='Province/State')\n",
    "g.set_title('Deaths across regions over time')\n",
    "g.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e4ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alternatively drop \"Others\" from the dataframe\n",
    "# Creating a new dataframe where any rows where the Province/State = 'Others' is dropped.\n",
    "# Selecting all \"Province/State\" except 'Others'.\n",
    "df2= df[df[\"Province/State\"].str.contains(\"Others\") == False]\n",
    "\n",
    "g=sns.lineplot(x=\"Month\", y=\"Deaths\", data=df2, hue='Province/State')\n",
    "g.set_title('Deaths across regions over time excl. Others')\n",
    "\n",
    "# Exporting the plot into a shareable PNG\n",
    "plt.savefig('Deaths_across_regions_ex_Others.png')\n",
    "\n",
    "# See that most other regions had very low deaths too as they still dont show properly.\n",
    "# Probably too many variables to plot in one chart. \n",
    "# Could be neat to split this into two charts.\n",
    "# Deaths have peaked and monthly aggregates in even the high death regions are still declining rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by Province/State, \n",
    "# Aggregate the count of recovered cases and sort the values of recovered cases in ascending order.\n",
    "# Creating the dataframe as above\n",
    "df=covid_clean.groupby(['Month','Province/State'], as_index=False)['Recovered'].count()\n",
    "\n",
    "# Visualising the output of df on a lineplot.\n",
    "g=sns.lineplot(x=\"Month\", y=\"Recovered\", data=df, hue='Province/State')\n",
    "g.set_title('Trend of Recovered Cases over the months')\n",
    "\n",
    "# Exporting the plot into a shareable PNG\n",
    "plt.savefig('Trend_Recovered_Cases_over_months.png')\n",
    "\n",
    "# Trends broadly the same across all Province/States.\n",
    "# Big increase in Recovered Cases after Feb 2021\n",
    "# Stable for the next few months.\n",
    "# A big drop in recovered cases since Sep 2021\n",
    "# Same time as slow rise in number of individuals who are eligible for the second dose who havent taken it yet. \n",
    "# Should look at what happens to cases reported over this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0137297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting related aggregated data to investigate for visual relationships between some variables\n",
    "# Explore impact of both doses being administered = Vaccinated on Total Deaths \n",
    "# Looks at aggregated numbers of Deaths, & administration of both first and second doses.\n",
    "# Use weekly data to get more granular view\n",
    "\n",
    "# Creating the dataframe\n",
    "df=covid_clean.groupby(['Week'], as_index=False)['Deaths', 'Vaccinated'].agg({'Deaths': 'sum',  \n",
    "                                                                               'Vaccinated': 'sum'})\n",
    "ax = df.plot(x=\"Week\", y=\"Deaths\", legend=False)\n",
    "ax2 = ax.twinx()\n",
    "df.plot(x=\"Week\", y=\"Vaccinated\", ax=ax2, legend=False, color=\"r\")\n",
    "ax.set_title('Chart3')\n",
    "ax.figure.legend()\n",
    "plt.show()\n",
    "\n",
    "# The variables Deaths & Vaccinated move together\n",
    "# Looks like there is a lagged impact which isnt coming across here due to X axis not being fully synchronised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9970dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are too many columns and rows for visual checks for correlations to clearly come through.\n",
    "# So many data points dont allow for particular relationships between variables being easily identified.\n",
    "# We can employ Python's statistical capabilities to get a quick over-view by running correlations.\n",
    "# The “corr()” method evaluates the correlation between all the variable, then it can be graphed with a color coding.\n",
    "\n",
    "# Prepare the dataframe\n",
    "# Drop  time parsing related & calculated variable columns which currently appear as independent variables in the dataframe.\n",
    "# This will remove obvious but spurious correlations obscuring the analysis. \n",
    "covid_clean2 = covid_clean.drop(['Week', 'Month', 'Year', 'Province/State', \n",
    "                                 'Part_Vac', 'First_Percentage', 'Ratio_of_Int' ], axis=1)\n",
    "\n",
    "# Setting up the correlation matrix\n",
    "corr = covid_clean2.corr()\n",
    "\n",
    "#Plotting the matrix.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Introducing heatmap\n",
    "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Setting up the display properties of the plot\n",
    "ticks = np.arange(0,len(covid_clean2.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(covid_clean2.columns)\n",
    "ax.set_yticklabels(covid_clean2.columns)\n",
    "\n",
    "# Visualise the correlation matrix.\n",
    "plt.show()\n",
    "\n",
    "# When there is no correlation between 2 variables (when correlation is 0 or near 0) the color is gray. \n",
    "# The darkest red means there is a perfect positive correlation.\n",
    "# Blue implies negative correlation.\n",
    "# The darkest blue means there is a perfect negative correlation.\n",
    "# The standout and rather obvious observation from this analysis is the positive correlation between Cases & Deaths.\n",
    "# There is also reasonably strong correlation between Vaccinated & First Dose.\n",
    "# This implies that those who have only taken the first dose shouldnt require too much persuasion to get double jabbed.\n",
    "# Would have been interesting to establish correlations between effect of vaccines and deaths as shown in Chart 3 above.\n",
    "# However this isnt coming up in the matrix which does not factor time lags as it stands here.\n",
    "# There is also a significant negative correlation between being fully Vaccinated and being hospitalised.\n",
    "# This may imply that the vaccine reduces the severity of the infection.\n",
    "# As such requirement to be hospitalised is reduced.\n",
    "# However there could also be other factors at play which might explain this seemingly symbiotic relationship.\n",
    "# Simple quantitative data analysis such as this wont reveal such factors.\n",
    "# For example, it could be that in the early days, medical professionals under pressure panicked & over-hospitalised patients.\n",
    "# Over time, as the panic subsided and doctors understood the infection better, they perhaps were more measured. \n",
    "# Thus hospitalisations dropped.\n",
    "# We need qualitative data to investigate this further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6892799",
   "metadata": {},
   "source": [
    "### Summaries & Observations from the Quantitative Data\n",
    "\n",
    "There are some strong, consistent themes with respect to timing as the variables; the key highlights:\n",
    "\n",
    "- There is a good amount of data albeit not real life credible; discussion on this aspect in the main report.\n",
    "- As vaccines get rolled out, death declines\n",
    "- The 'risky' variables appear to move in tandem with changes in vaccine uptake & numbers of fully vaccinated.\n",
    "- The Province/State 'Others' should be stripped out for further study away from this exercise.\n",
    "- The data is so vastly different in order of magnitude, its creating skews which are masking potential meaningful insights.\n",
    "- The difference in magnitude makes meaningful comparative studies futile.\n",
    "- One way to resolve this may be to aggregate all other various regions and then set up a comparative study with 'Others'.\n",
    "- This may mask other important nuances of certain regions.\n",
    "- Another method to resolve this problem of scale could be to normalise the whole data set.\n",
    "- On normalisation, all variable values would fall within a predetermined range of values.\n",
    "- Converting the data from daily to months made studying the data as time-series much easier.\n",
    "- Daily observations are too noisy.\n",
    "- Deaths have peaked in the summer of 2021 although they appear to be rising again.\n",
    "- Both Deaths and Recovereds show very similar bi-modal distribution characteristics.\n",
    "- Indicates there were two peaks over the summer of 2021.\n",
    "- No idiosyncratic regional trends discovered yet.\n",
    "- St. Helena, Ascension & Tristan da Cunha may be marginally better at recoveries although it wasnt so at the start of the pandemic.\n",
    "- Most visualisations need a lot more work in terms of quality of presentation.\n",
    "- The correlation matrix is a neat tool but assumes some basic statistical knowledge & may not be suitable for all audiences.\n",
    "- There are many problems with this data set in terms of real life credibility; as such correlations should be treated with caution. They are intended to give a sense of trends and themes to the audience & not for use in decision making at this stage.\n",
    "- Visualisations alone wont be sufficient to make decisions but should assist in generating the big picture storyboard in the minds of the government.\n",
    "- There should be multivariate, spatial amongst other statistical analysis done on the data to generate actionable insights once the real world credibility of the underlying data is resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815bfb8",
   "metadata": {},
   "source": [
    "## Analysing Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3518f42e",
   "metadata": {},
   "source": [
    "#### Further look at the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ad4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values containing the # symbol.\n",
    "tweets.loc[tweets['text'] != '#']\n",
    "\n",
    "# Every single row contains at least one # symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242af9f",
   "metadata": {},
   "source": [
    "### Some basic descriptive analysis of the dataset tweets.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51febcbc",
   "metadata": {},
   "source": [
    "#### Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef50b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Number of times a tweet is retweeted.\n",
    "tweets = tweets.sort_values(by='retweet_count', ascending=False)\n",
    "tweets = tweets.reset_index(drop=True)\n",
    "\n",
    "# Displaying results to nearest whole number\n",
    "round(tweets['retweet_count'].mean(),0)\n",
    "\n",
    "# We see than on average, a tweet is re-tweeted a further 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting how many times a tweet has been re-tweeted.\n",
    "# We have to revisit the original dataframe loaded from csv file\n",
    "# Recall re_tweet count was dropped earlier in this exercise \n",
    "rt=tweets['retweet_count'] > 0\n",
    "rt.value_counts()\n",
    "\n",
    "# Shows that 1142 tweets were retweeted at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of likes \n",
    "tweets = tweets.sort_values(by='favorite_count', ascending=False)\n",
    "tweets = tweets.reset_index(drop=True)\n",
    "\n",
    "# Displaying results to nearest 2 decimals.\n",
    "round(tweets['favorite_count'].mean(),2)\n",
    "\n",
    "# We can see that an average tweet is marked favourite 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fad56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times a tweet has been made favourite.\n",
    "fav=tweets['favorite_count'] > 0\n",
    "fav.value_counts()\n",
    "\n",
    "# Shows that 1720 tweets were marked as favourite by someone at least once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29e7fc",
   "metadata": {},
   "source": [
    "#### Top trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top trending hashtags.\n",
    "# Ranked & limited to top 30 hashtags.\n",
    "# These are the most frequently occuring string of texts that are either preceeded or followed by a '#'.\n",
    "# To search for words starting with # we use (#.*?).\n",
    "# To search for words or sentences ending #  (?=\\s|$).\n",
    "\n",
    "tweets2.text.str.findall(r'#.*?(?=\\s|$)').head(31)\n",
    "\n",
    "# This output isnt very useful. \n",
    "# Very quick scan of output does show that Covid 19 & coronavirus were very popular.\n",
    "# # But cant make sense of how popular from the output in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f424cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe that contains just the text column from the tweets df.\n",
    "# Converting the datatype in column 'text' as string.\n",
    "tweets['text'] = tweets['text'].astype(str)\n",
    "\n",
    "# Creating the dataframe.\n",
    "tweets_text = tweets['text'].apply(lambda x: x if x.strip() != None else none)\n",
    "\n",
    "# From the new df, creating a dict/list of all values with '#' in it.\n",
    "tags = [x for x in tweets_text if x.startswith('#')]\n",
    "for y in [x.split(' ') for x in tweets_text.values]:\n",
    "    for z in y:\n",
    "        if '#' in z:\n",
    "            tags.append(z)\n",
    "\n",
    "\n",
    "            \n",
    "#Create a Series from the dictionary\n",
    "tags=pd.Series(tags).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01363d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify 30 most popular hashtags.\n",
    "tags.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into a dataframe\n",
    "df = pd.DataFrame(tags).reset_index()\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0215a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df.columns = ['word', 'count']\n",
    "df['count'] = df['count'].astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hashtags which have been tweeted at least 100 times.\n",
    "# This will further filter for significance of these hashtags.\n",
    "df[df['count']>100]\n",
    "\n",
    "# Can see actually very few really popular hashtags.\n",
    "# Top 5 are all Covid-19 related.\n",
    "# We should actually look for distinct words as a lot of the count is distorted by syntax.\n",
    "# No real difference between COVID19, Covid19 & covid19 for example.\n",
    "# Only distinct hashtags are Covid19, CovidIsNotOver, China and Greece.\n",
    "# What was happening in Greece around this time?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3bb30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising the table above.\n",
    "g = sns.barplot(x=\"count\", y=\"word\", data=df.loc[(df['count']>100)])\n",
    "g.set_title('Top Trending Hashtags')\n",
    "\n",
    "# Very few meaningful insights here.\n",
    "# Dataset is too small and over a very short period of time.\n",
    "# Could see how these top trending hashtags evolve over the whole period of our quantitative analysis earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2b806",
   "metadata": {},
   "source": [
    "### Summaries & Observations from Qualitative Data\n",
    "\n",
    "The dataset provided is too small to provide meaningful insights.  Despite this, there are a handful strong, consistent themes that are worthwhile pointing out:\n",
    "\n",
    "- Assuming that the data provided was from a period that was randomly selected, Twitter should be medium that the government targets to get its messaging on COVID across. \n",
    "- There is a significant level of animation amongst Twitter users in relation to COVID-19.\n",
    "- Within the data provided, COVID was the highest trending hashtag by a great magnitude.\n",
    "- Any government communication on the topic on Twitter will get a lot of eyeballs & can be thus very effective.\n",
    "- Additional sentiment analysis would be a very worthwhile endeavour on a bigger dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1297a6",
   "metadata": {},
   "source": [
    "### Week 6 Assignment\n",
    "The government employed an external consultant to assist with data analysis. However, the consultant resigned and left the project in a half-completed state. The government provided you with the partially completed Jupyter Notebook that the consultant developed. They asked you to evaluate and complete the Jupyter Notebook with Python code.\n",
    "\n",
    "This assignment activity will consist of two parts: (1) demonstrate the use of the functions provided, and (2) answer additional questions posed by the government. The questions are:\n",
    "\n",
    "- What is the difference between qualitative and quantitative data? How can these be used in business predictions?\n",
    "- Why is continuous improvement required? Can we not just implement the project and move on to other pressing matters?\n",
    "- As a government, we adhere to all data protection requirements and have good governance in place. We only work with aggregated data and therefore will not expose any personal details. Does that mean we can ignore data ethics?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93dcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a special dataframe for the Province/State = Channel Islands\n",
    "ds1 = pd.read_csv('covid_19_uk_cases.csv')\n",
    "ds2 = pd.read_csv('covid_19_uk_vaccinated.csv')\n",
    "\n",
    "sample = ds1[['Province/State','Date','Hospitalised']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b523d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data for a specific province\n",
    "sample_ci = sample[sample['Province/State'] == \"Channel Islands\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to plot moving averages\n",
    "def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n",
    "    \n",
    "    # Create a rolling window to calculate the rolling mean using the series.rolling function\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "    \n",
    "    # Declare the dimensions for the plot, plot name and plot the data consisting of the rolling mean from above \n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.title('Moving average\\n window size = {}'.format(window))\n",
    "    plt.plot(rolling_mean, 'g', label='Simple moving average trend')\n",
    "\n",
    "    \n",
    "    # Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        \n",
    "        # Calculate the mean absolute square \n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        \n",
    "        # Calculate the standard deviation using numpy's std function\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        \n",
    "        # Calculate the upper and lower bounds \n",
    "        lower_bound = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bound = rolling_mean + (mae + scale * deviation)\n",
    "        \n",
    "        # Name and style upper and lower bound lines and labels \n",
    "        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n",
    "        plt.plot(lower_bound, 'r--')\n",
    "    \n",
    "    # Plot the actual values for the entire timeframe\n",
    "    plt.plot(series[window:], label='Actual values')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa199f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate the mean absolute error\n",
    "def mean_absolute_error(a, b): return abs(b - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce296c",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "The consultant indicated that the function is functional, but did not demonstrate how to use it. Use the provided function, `plot_moving_average()`, to plot the data for the selected province (variable name is `sample_ci` and set the window parameter to 7 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f39d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the use of the function to plot moving averages.\n",
    "\n",
    "# First we need to ensure that we have all the environment set up.\n",
    "# I have added a couple of libraries I need to the start cell.\n",
    "# Converting the date column into datetime object.\n",
    "# This is called parsing. \n",
    "# This is needed to calculate moving averages.\n",
    "sample_ci['Date'] = pd.to_datetime(sample_ci['Date'], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Calculating the 7 day moving average of Hospitalised numbers\n",
    "# Inserting this calculated value as a new column within the existing dataframe, sample_ci\n",
    "sample_ci['7d_MovingAvg'] = ((sample_ci['Hospitalised']).rolling(window=7).mean())\n",
    "\n",
    "# We can change the number in the function above 'window=7' any other number that we wish to have to calculate moving average.\n",
    "# For example if we wanted 20 day moving average, it would read window=5 in the function above.\n",
    "\n",
    "# Now plotting this into a graph to show the 7 day moving average of the number of Hospitalisations in the Channel Islands.\n",
    "# X Axis is time = 'Date'\n",
    "# Y Axis = 7 day moving average of 'Hospitalised'\n",
    "\n",
    "# Setting size of the plot.\n",
    "plt.figure(figsize = (20,8))\n",
    "\n",
    "# Visualising the graph.\n",
    "g=sns.lineplot(x=\"Date\", y=\"7d_MovingAvg\", data=sample_ci)\n",
    "\n",
    "# Set out chart title.\n",
    "g.set_title('7day moving average of number of Hospitalisations in the Channel Islands')\n",
    "\n",
    "# Moving averages are useful to identify the trend direction of a variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60aa9bd",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "The consultant performed a calculation that looks interesting, but the team is unsure about what the intention was and how to interpret the output. Can you offer some insights into the meaning of the code and output in the cell below? Is it useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f491bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the top three days with biggest difference between daily value and rolling 7-day mean\n",
    "s = sample_ci.copy()\n",
    "s_rolling = s['Hospitalised'].rolling(window=7).mean()\n",
    "s['error'] = mean_absolute_error(s['Hospitalised'][7:], s_rolling[7:])\n",
    "s.sort_values('error', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf044de",
   "metadata": {},
   "source": [
    "- The above function calculates the 7day moving average of Hospitalisations in the Channel Islands.\n",
    "- It also displays the actual number of Hospitalisations on the day. \n",
    "- Together, the two numbers are useful to identify the trend in which the actual number is going to move in the future. \n",
    "- If the moving average is less than the actual number on the day, it would typically signal that the actual numbers are on an uptrend and thus likely to continue to increase. There are other factors which may be used to better understand the signal provided by this difference between the moving average and the actual number on the day. However, this is out of scope for this assignment.\n",
    "- The mean absolute error as shown in the Column 'error' in the table above shows the accuracy/strength of the signalling generated by the moving average as described above.\n",
    "- The output in the table has been sorted in descending order of mean absolute error.\n",
    "- Displayed in this manner, we can very easily identify the days when the predictive power of the 7d Moving Average was at its least capable. \n",
    "- Beyond this, displaying it in this manner has no value. Typically, given such studies are carried out on very large datasets, you would not display the output in this manner except to quickly check the worst case predictive power of the forecasting tool. \n",
    "- Output of this variety is very useful for predictive technologies especially for time series forecasting. It has little value in static data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a34adb",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "The management team had some additional questions around the project where they asked for further feedback to be included in your final presentation. Make sure to answer the questions in the Notebook in Markdown format in preparation for your presentation. The expectation is that you will provide short and direct responses to help them understand the importance and impact of the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40daa1",
   "metadata": {},
   "source": [
    "#### What is the difference between qualitative and quantitative data? How can these be used in business predictions?\n",
    "\n",
    "Quantitative data is numbers based and therefore measured. Qualitative data is descriptive and whilst it can be observed, there is no way to measure them. The techniques used for the collection and analysis is unique to each data type although certain subsets are shared.\n",
    "\n",
    "Both data types have their place in being used for business predictions. Quantitative data is typically more readily available and thus useful for analysis on the fly. However, it can lack depth and on a stand-alone basis provides no context. Put simply, whilst quantitative data is useful for facts, qualitative data is a powerful story telling tool whilst keeping that story anchored in the real world. \n",
    "\n",
    "If you are running a business, it is crucial to know not just how your business is performing financially but also within its wider eco system with respect to customer and employee engagements and environmental impact amongst other measures.\n",
    "\n",
    "For further discussion on this topic, I have found [this article](https://www.linkedin.com/pulse/20140920165433-34529931-qualitative-quantitative-decision-making/) to be particularly helpful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03208cd",
   "metadata": {},
   "source": [
    "#### Can you provide you observations around why continuous improvement is required, can we not just implement the project and move on to other pressing matters?\n",
    "\n",
    "Continuous improvement techniques are used as a strategy to proactively make consistent and iterative enhancements. These techniques can be used to improve products, processes and people. Simply put, it's the best way to ensure we are always at the boundaries of efficiency and knowledge . If continuous improvement processes are not in place and the focus instead is on task deliveries, over time productivity and market value will decline. Afterall, Rome wasn't built in a day but it did burn to the ground almost overnight!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55801f65",
   "metadata": {},
   "source": [
    "#### As a government, we adhere to all data protection requirements and have good governance in place. Does that mean we can ignore data ethics? We only work with aggregated data and therefore will not expose any personal details? (Provide an example of how data ethics could apply to this case; two or three sentences max)\n",
    "\n",
    "Data ethics whilst nuanced and complicated, cannot be ignored by any organisation that is not beyond the reach of the law and/or public opinion. Data can be supremely powerful but with great power comes great responsibility; primarily to ensure that the data is used for the appropriate use cases. \n",
    "\n",
    "In this specific case, using public health data which is ultimately aggregated from personal data can:\n",
    "\n",
    "- cause further damage to public health and the economy if policy is formed basis erroneous data techniques.\n",
    "\n",
    "- lead to breaches in confidentiality guaranteed by the law if the data is not handled appropriately. Ultimately, this could lead to legal action being taken against the government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b34b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9b5d5ccfa2a39a1a0d2f36563eb706df95703394a033b14f208f93362b3d32f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
